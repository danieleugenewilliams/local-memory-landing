# Local Memory Landing Page Copy

## Hero Section

### Main Headline
**The AI Memory System That Actually Works**

### Subheading  
**Enterprise performance, developer price. 5-8x faster vector search with native MCP integration.**

### Key Benefits (3-column layout)
```
⚡ 11ms Response Times          🔧 26+ Native MCP Tools         🌐 Universal Compatibility
Production-grade performance   Appear directly in Claude       Works with any AI platform
5-8x faster than alternatives  Desktop & Code                  MCP + REST API support
```

### Primary CTA
```
[🚀 Get Early Access - $29] [📊 View Benchmarks]
```

### Supporting Text
*One-time purchase • Zero dependencies • Works offline • 2-minute setup*

---

## Performance Leadership Section

### Headline
**Enterprise Performance You Can Actually Measure**

### Performance Grid
```
| Metric              | Before    | After     | Improvement        |
|---------------------|-----------|-----------|-------------------|
| Vector Search       | 50-100ms  | 11-15ms   | 🚀 5-8x FASTER    |
| Full-text Search    | 30-50ms   | 20ms      | 🚀 2-3x FASTER    |
| Tag Search          | 25ms      | 12ms      | 🚀 2x FASTER      |
| API Response        | 30ms      | 15-20ms   | 🚀 1.5-2x FASTER  |
```

### Supporting Copy
"Engineered for **production-grade performance** with ±1ms consistency. While other memory systems struggle with basic queries, local-memory handles high-volume workloads with enterprise reliability."

### Technical Badge
*✅ Performance Grade: A+ • Ready for production workloads*

---

## Universal Compatibility Section

### Headline
**Works With Every AI Platform (Seriously)**

### Platform Grid
```
🖥️ Claude Desktop          💻 Claude Code             🌐 Any AI Platform
Native MCP Integration     MCP + REST API Support     Universal REST API
26+ tools appear directly   Terminal & web integration  OpenCode, ChatGPT, custom agents
```

### Code Examples Tabs

#### Tab 1: Claude Desktop (MCP)
```json
{
  "mcpServers": {
    "local-memory": {
      "command": "/usr/local/bin/local-memory",
      "args": []
    }
  }
}
```

#### Tab 2: Claude Code (MCP)
```bash
claude mcp add local-memory /path/to/local-memory-mcp
```

#### Tab 3: Universal REST API
```javascript
// Works with any AI platform
const response = await fetch('http://localhost:3002/memories/search', {
  method: 'POST',
  body: JSON.stringify({
    query: "authentication patterns",
    use_ai: true,
    limit: 5
  })
});
```

### Supporting Copy
"**Future-proof architecture.** Native MCP integration for modern AI agents, universal REST API for everything else. The only memory system designed for both protocols."

---

## Developer-First Design Section

### Headline
**Built by Developers, for Developers**

### Feature Grid (3 columns)
```
🏗️ Zero Dependencies           ⚡ Memory Efficient         🛠️ Production Ready
Single binary                 16MB RAM usage              SQLite + Qdrant support
No Node.js, Python, Docker    vs 200MB+ alternatives     Health monitoring included
Cross-platform support        Automatic DB creation       Graceful error handling
```

### Quick Start
```
# Download, extract, run
local-memory start

# 2 minutes to full AI memory
✅ Database created automatically
✅ Vector search optimized  
✅ 26+ MCP tools ready
✅ REST API listening on :3002
```

### Developer Benefits
- **Persistent Expertise**: AI agents remember your codebase, patterns, and solutions
- **Cross-Session Learning**: Knowledge builds up over time, not lost between conversations  
- **Smart Context Retrieval**: Vector search finds relevant context automatically
- **Team Knowledge Base**: Shared memory across your development team

---

## Pricing Section

### Headline
**Premium Performance, Indie Price**

### Pricing Card
```
🎯 Early Access Special
 
$29 (normally $49)
━━━━━━━━━━━━━━━━━━━
✓ Complete local-memory system
✓ All 26+ MCP tools included
✓ Unlimited usage forever
✓ Priority email support
✓ Future updates included

[🚀 Get Early Access Now]

⏰ Limited time: First 1000 developers
💰 One-time purchase, no subscriptions
🎁 $10 credit for each referral
```

### Value Comparison
*"Costs less than 2 hours of development time. Saves hundreds of hours of re-explaining context to AI agents."*

### FAQ Preview
```
❓ What if I don't like it?
💡 30-day money-back guarantee

❓ Do I need anything else installed?  
💡 Only Ollama for AI features (optional)

❓ Does it work offline?
💡 Completely offline after setup
```

---

## Social Proof Section

### Headline
**Performance That Speaks for Itself**

### Benchmark Highlight
```
🏆 BENCHMARK RESULTS
"Outstanding performance improvements across all metrics"
- 5-8x faster vector search
- Enterprise-grade consistency (±1ms variation)
- Ready for high-volume production workloads
```

### Technical Validation
*"Performance-optimized architecture delivers exceptional results. These benchmark numbers outclass existing solutions while maintaining full compatibility."*

### Community Badge
*🥇 **Founding Developer Program** - First 100 customers get priority support and exclusive updates*

---

## Call-to-Action Section

### Headline
**Join the Memory Revolution**

### Final Pitch
"Stop re-explaining your codebase to AI agents. Give them persistent memory with enterprise performance."

### Dual CTA
```
[🚀 Get Early Access - $29]    [📖 Read Documentation]
40% off • Limited time          Technical specs & setup guide
```

### Trust Signals
```
✅ 30-day money-back guarantee
✅ Secure one-time payment via Stripe  
✅ Instant download after purchase
✅ Email support included
```

### Final Hook
*"The only AI memory system with native MCP integration AND universal REST API. Future-proof your AI workflow today."*

---

## Technical Specifications (Expandable Section)

### System Requirements
- **Platforms**: macOS (Intel/Apple Silicon), Linux, Windows
- **Memory**: 16MB typical usage (8GB+ recommended for Ollama)
- **Storage**: 50MB binary + vector database (grows with usage)
- **Network**: Only for initial Ollama model download (optional)

### Technical Features
- **Protocols**: MCP (Model Context Protocol) + REST API
- **Storage**: SQLite database with optional Qdrant vector support
- **Search**: Vector similarity + full-text + semantic AI search
- **Performance**: Sub-20ms response times, ±1ms consistency
- **Integration**: 26+ MCP tools, 20+ REST endpoints

### Architecture Benefits
- **No Dependencies**: Single binary, no runtime requirements
- **Offline Capable**: Works completely offline after setup
- **Memory Efficient**: 16MB vs 200MB+ Node.js alternatives
- **Production Ready**: Health monitoring, graceful error handling